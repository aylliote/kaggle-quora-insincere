{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Some imports\n",
    "\n",
    "# python built-in imports\n",
    "import json, re, gc, time, os\n",
    "from collections import defaultdict\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(category=FutureWarning, action=\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# keras imports\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Embedding, Dropout, Dense,\n",
    "                          Concatenate, GlobalMaxPooling1D,\n",
    "                          SpatialDropout1D, Bidirectional,\n",
    "                          CuDNNLSTM, CuDNNGRU)\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# progress-bar imports\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas('my Bar ! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Translate LaTeX in texts\n",
    "LATEX_DICT = {r'\\\\mathrm': ' LaTex math mode ', r'\\\\mathbb': ' LaTex math mode ',\n",
    "  r'\\\\boxed': ' LaTex equation ', r'\\\\begin': ' LaTex equation ', r'\\\\end': ' LaTex equation ',\n",
    "  r'\\\\left': ' LaTex equation ', r'\\\\right': ' LaTex equation ', r'\\\\(over|under)brace': ' LaTex equation ',\n",
    "  r'\\\\text': ' LaTex equation ', r'\\\\vec': ' vector ', r'\\\\var': ' variable ', r'\\\\theta': ' theta ',\n",
    "  r'\\\\mu': ' average ', r'\\\\min': ' minimum ', r'\\\\max': ' maximum ', r'\\\\sum': ' + ', r'\\\\times': ' * ',\n",
    "  r'\\\\cdot': ' * ', r'\\\\hat': ' ^ ', r'\\\\frac': ' / ', r'\\\\div': ' / ', r'\\\\sin': ' Sine ', r'\\\\cos': ' Cosine ',\n",
    "  r'\\\\tan': ' Tangent ', r'\\\\infty': ' infinity ', r'\\\\int': ' integer ', r'\\\\in': ' in '}\n",
    "\n",
    "RE_LATEX = re.compile('(%s)' % '|'.join(LATEX_DICT.keys()))\n",
    "LATEX_DICT = {k.strip('\\\\'): v for k, v in LATEX_DICT.items()}\n",
    "\n",
    "\n",
    "## Some RegExs to filter the texts\n",
    "RE_URL = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\")\n",
    "RE_ALPHA_NUMERIC = re.compile(r'[^\\w *+?!-]')\n",
    "RE_REPEATS = re.compile(r\"([A-Za-z])\\1{2,}\", re.DOTALL)\n",
    "RE_NUMBERS = re.compile(r'[0-9]{2,}', re.DOTALL)\n",
    "RE_NON_ASCII = re.compile(r'^[\\u0080-\\uFFFF]+$')\n",
    "\n",
    "\n",
    "## Composed words to be splitted\n",
    "TO_SPLIT = set([w.strip() for w in open('to_split.txt', 'r', encoding='utf8').readlines()])\n",
    "\n",
    "\n",
    "# Annoying exadecimal characters\n",
    "hex_chars = ['\\x00', '\\x01', '\\x02', '\\x03', '\\x04', '\\x05', '\\x06', '\\x07',\n",
    "             '\\x08', '\\x09', '\\x0a', '\\x0b', '\\x0c', '\\x0d', '\\x0e', '\\x0f',\n",
    "             '\\x10', '\\x11', '\\x12', '\\x13', '\\x14', '\\x15', '\\x16', '\\x17',\n",
    "             '\\x18', '\\x19', '\\x1a', '\\x1b', '\\x1c', '\\x1d', '\\x1e', '\\x1f']\n",
    "\n",
    "\n",
    "def replace_latex(match):\n",
    "    try:\n",
    "        word = LATEX_DICT.get(match.group(0).strip('\\\\'))\n",
    "    except KeyError:\n",
    "        word = match.group(0)\n",
    "    return word\n",
    "\n",
    "\n",
    "def clean_latex(text):\n",
    "    text = re.sub(r'\\[math\\]', ' LaTex math ', text)\n",
    "    text = re.sub(r'\\[\\/math\\]', ' LaTex math ', text)\n",
    "\n",
    "    return RE_LATEX.sub(replace_latex, text)\n",
    "\n",
    "\n",
    "def replace_numbers(match):\n",
    "    try:\n",
    "        n = match.group(0)\n",
    "        if n == \"11\":\n",
    "            return n\n",
    "        n = min(len(n), 7)\n",
    "        return \" 1\" + \"0\" * (n - 1) + \" \"\n",
    "    except:\n",
    "        return \"number\"\n",
    "\n",
    "\n",
    "def clean_numbers(text):\n",
    "    text = RE_NUMBERS.sub(replace_numbers, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def strip_non_ascii(s):\n",
    "    return RE_NON_ASCII.sub('<NON_ASCII>', s)\n",
    "\n",
    "\n",
    "# Loads spelling corrections stored on the Disk\n",
    "def load_dicts():\n",
    "    wmaps = dict()\n",
    "\n",
    "    def load_json(path):\n",
    "        with open(path, 'r', encoding='utf8') as fp:\n",
    "            data = json.load(fp)\n",
    "        return data\n",
    "\n",
    "    for file in os.listdir('./'):\n",
    "        if file.endswith('.json'):\n",
    "            wmaps[file.split('.')[0]] = load_json(file)\n",
    "            \n",
    "    return wmaps\n",
    "\n",
    "\n",
    "# Sort keys such that substrings are well ordered\n",
    "def sort_keys(wmap):\n",
    "    keys = list(wmap.keys())\n",
    "    keys = sorted(keys, key=lambda k: -len(k))\n",
    "    return keys\n",
    "\n",
    "\n",
    "wmaps = load_dicts()\n",
    "\n",
    "keys_emojis = sort_keys(wmaps[\"emojis\"])\n",
    "keys_first_order = sort_keys(wmaps[\"first_order_corrections\"])\n",
    "keys_second_order = sort_keys(wmaps[\"second_order_corrections\"])\n",
    "keys_lemmas = sort_keys(wmaps[\"lemmas_corrections\"])\n",
    "\n",
    "# Loop over keys, word_maps\n",
    "keys_wmaps_iterator = list(zip([keys_emojis, keys_first_order,\n",
    "                           keys_second_order, keys_lemmas],\n",
    "                           [wmaps[\"emojis\"], wmaps[\"first_order_corrections\"],\n",
    "                           wmaps[\"second_order_corrections\"], wmaps[\"lemmas_corrections\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(s):\n",
    "    s = RE_URL.sub(' URL ', s)\n",
    "    s = re.sub('’', \"'\", s)\n",
    "    s = re.sub(\"' \", \"'\", s)\n",
    "\n",
    "    s = clean_latex(s)\n",
    "\n",
    "    for hc in hex_chars:\n",
    "        s = s.replace(hc, ' ')\n",
    "\n",
    "    # Remove repeatitions\n",
    "    s = RE_REPEATS.sub(r\"\\1\" * 2, s)\n",
    "      \n",
    "    for keys, wmap in keys_wmaps_iterator:\n",
    "        for word in keys:\n",
    "            s = s.replace(word, wmap[word])\n",
    "\n",
    "    s = RE_ALPHA_NUMERIC.sub(' ', s).replace('_', '')\n",
    "    s = \" \".join(s.split()) + \" \"\n",
    "    s = s.replace(' 9 11 ', ' 9/11 ')\n",
    "    s = s.replace(\" s \", \" 's \")\n",
    "    s = clean_numbers(s)\n",
    "\n",
    "    tokens = sum([t.split('-') if t in TO_SPLIT else [t]\n",
    "                  for t in s.split()], [])\n",
    "\n",
    "    tokens = [strip_non_ascii(t) for t in tokens]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Adapted from keras tokenizer Object\n",
    "class MyTokenizer:\n",
    "    def __init__(self, maxwords):\n",
    "        self.word_index = defaultdict(int)\n",
    "        self.maxwords = maxwords\n",
    "        self.oov = None\n",
    "        self.pad = 0\n",
    "        \n",
    "    def fit(self, docs):\n",
    "        for doc in docs:\n",
    "            for word in doc:\n",
    "                self.word_index[word] += 1\n",
    "        \n",
    "        self.word_index = sorted(self.word_index.items(),\n",
    "                        key=lambda k : -k[1])[:self.maxwords]\n",
    "\n",
    "        self.word_index = {w: ix + 1 for ix, (w, _) in\n",
    "                           enumerate(self.word_index)}\n",
    "        \n",
    "        self.word_index['<OOV>'] = self.oov = max(self.word_index.values()) + 1\n",
    "\n",
    "    def to_sequences(self, docs, maxlen):\n",
    "        seqs = [self.__index_and_pad(d, maxlen=maxlen) for d in docs]\n",
    "\n",
    "        return seqs\n",
    "\n",
    "    def __index_and_pad(self, tokens, maxlen):\n",
    "        seq = [self.word_index.get(t, self.oov) for t in tokens]\n",
    "        seq = seq + [self.pad] * (maxlen - len(seq)) if len(seq) < maxlen else seq[:maxlen]\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Loads embeddings from filepath and compute embedding matrix\n",
    "def load_embeddings(word_index, embedding_file, corrections):\n",
    "    embedding_dim = 300\n",
    "    nb_words = len(word_index)+1\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    f = open(embedding_file, 'r', encoding='utf8', errors=\"ignore\")\n",
    "    for line in tqdm(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == embedding_dim + 1 and word in word_index:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    embeddings_index['<NON_ASCII>'] = np.random.normal(-0.005838499, 0.48782197, (embedding_dim,))\n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "    oov_vector = np.zeros((embedding_dim,), dtype=np.float32) - 1.\n",
    "    \n",
    "    for word, i in word_index.items():\n",
    "        if word == \"<OOV>\":\n",
    "            embedding_matrix[i] = oov_vector\n",
    "            continue\n",
    "        \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embeddings_index.get(word.lower())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embeddings_index.get(word.upper())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embeddings_index.get(word.capitalize())\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "        corr = corrections.get(word)\n",
    "        if corr is not None:\n",
    "            embedding_vector = embeddings_index.get(corr)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                continue\n",
    "        \n",
    "        embedding_matrix[i] = oov_vector\n",
    "        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Search optimal threshold for predictions\n",
    "def threshold_search(y_true, y_proba):\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "    for threshold in GRID:\n",
    "        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    search_result = {'threshold': best_threshold, 'f1': best_score}\n",
    "    return search_result\n",
    "\n",
    "## Build Neural model\n",
    "def get_lstm_model():\n",
    "    inputs = Input(shape=(MAX_LEN,), dtype='int32')\n",
    "    embedding = Embedding(input_dim=MAX_WORDS,\n",
    "                                output_dim=EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=False)(inputs)\n",
    "    embedding = SpatialDropout1D(0.3)(embedding)\n",
    "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(embedding)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    predictions = Dense(1, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded in 00m03s\n",
      "Process dataset ...\n",
      "\n",
      "100%|██████████| 1681928/1681928 [06:04<00:00, 4614.53it/s]\n",
      "\n",
      "Dataset successfully processed\n",
      "Tokenize corpus ...\n",
      "Corpus successfully tokenized\n",
      "All words : 100001\n",
      "Load GLOVE embeddings ...\n",
      "\n",
      "2196017it [00:59, 36810.86it/s]\n",
      "\n",
      "Load PARAGRAM embeddings ...\n",
      "\n",
      "1703756it [00:47, 35916.62it/s]\n",
      "\n",
      "Embedding matrix ready\n",
      "Compute text sequences ...\n",
      "Sequences ready\n",
      "Remove garbage ...\n",
      "Starts training ...\n",
      "\n",
      "MODEL: 1\n",
      "Epoch 1/4\n",
      "1142856/1142856 [==============================] - 198s 173us/step - loss: 0.1116 - acc: 0.9564\n",
      "Epoch 2/4\n",
      "1142856/1142856 [==============================] - 195s 170us/step - loss: 0.0979 - acc: 0.9610\n",
      "Epoch 3/4\n",
      "1142856/1142856 [==============================] - 194s 170us/step - loss: 0.0911 - acc: 0.9635\n",
      "Epoch 4/4\n",
      "1142856/1142856 [==============================] - 194s 170us/step - loss: 0.0845 - acc: 0.9658\n",
      "\n",
      "Th : 0.929  -->  F1 : 0.69678 \n",
      "\n",
      "MODEL: 2\n",
      "Epoch 1/4\n",
      "1142856/1142856 [==============================] - 195s 171us/step - loss: 0.1114 - acc: 0.9562\n",
      "Epoch 2/4\n",
      "1142856/1142856 [==============================] - 194s 170us/step - loss: 0.0976 - acc: 0.9609\n",
      "Epoch 3/4\n",
      "1142856/1142856 [==============================] - 194s 170us/step - loss: 0.0908 - acc: 0.9634\n",
      "Epoch 4/4\n",
      "1142856/1142856 [==============================] - 195s 170us/step - loss: 0.0845 - acc: 0.9660\n",
      "\n",
      "Th : 0.932  -->  F1 : 0.69257 \n",
      "\n",
      "MODEL: 3\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 196s 171us/step - loss: 0.1118 - acc: 0.9560\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0978 - acc: 0.9611\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0909 - acc: 0.9635\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0843 - acc: 0.9662\n",
      "\n",
      "Th : 0.930  -->  F1 : 0.69076 \n",
      "\n",
      "MODEL: 4\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 196s 171us/step - loss: 0.1119 - acc: 0.9560\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0978 - acc: 0.9607\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0909 - acc: 0.9634\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0848 - acc: 0.9657\n",
      "\n",
      "Th : 0.927  -->  F1 : 0.69655 \n",
      "\n",
      "MODEL: 5\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 196s 172us/step - loss: 0.1117 - acc: 0.9559\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0977 - acc: 0.9609\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0909 - acc: 0.9635\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0843 - acc: 0.9661\n",
      "\n",
      "Th : 0.927  -->  F1 : 0.69255 \n",
      "\n",
      "MODEL: 6\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.1114 - acc: 0.9563\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 194s 170us/step - loss: 0.0978 - acc: 0.9610\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0911 - acc: 0.9634\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0845 - acc: 0.9658\n",
      "\n",
      "Th : 0.927  -->  F1 : 0.69246 \n",
      "\n",
      "MODEL: 7\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 196s 171us/step - loss: 0.1120 - acc: 0.9560\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0979 - acc: 0.9609\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0908 - acc: 0.9636\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0843 - acc: 0.9661\n",
      "\n",
      "Th : 0.931  -->  F1 : 0.69228 \n",
      "\n",
      "MODEL: 8\n",
      "Epoch 1/4\n",
      "1142857/1142857 [==============================] - 196s 172us/step - loss: 0.1118 - acc: 0.9559\n",
      "Epoch 2/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0977 - acc: 0.9610\n",
      "Epoch 3/4\n",
      "1142857/1142857 [==============================] - 195s 170us/step - loss: 0.0909 - acc: 0.9636\n",
      "Epoch 4/4\n",
      "1142857/1142857 [==============================] - 195s 171us/step - loss: 0.0842 - acc: 0.9661\n",
      "\n",
      "Th : 0.931  -->  F1 : 0.69350 \n",
      "\n",
      "Finished training!\n",
      "Computes optimal threshold ...\n",
      "Found optimal threshold 0.931\n",
      "Prepare submission.csv ...\n",
      "\n",
      "\n",
      " DONE ! Total run-time : 01h57m58s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "df = pd.concat([pd.read_csv('../input/train.csv'),\n",
    "                pd.read_csv('../input/test.csv')],\n",
    "               axis=0).reset_index(drop=True)\n",
    "\n",
    "print(\"Dataset loaded in {:02d}m{:02d}s\".format(*divmod(int(time.time() - t0), 60)))\n",
    "print(\"Process dataset ...\")\n",
    "time.sleep(0.5)\n",
    "\n",
    "docs = df.question_text.progress_apply(process)\n",
    "docs = docs.tolist()\n",
    "\n",
    "print('Dataset successfully processed')\n",
    "print('Tokenize corpus ...')\n",
    "\n",
    "MAX_WORDS = 100000\n",
    "tokenizer = MyTokenizer(maxwords=MAX_WORDS)\n",
    "tokenizer.fit(docs)\n",
    "word_index = tokenizer.word_index\n",
    "print('Corpus successfully tokenized')\n",
    "print('All words : %d' % len(word_index))\n",
    "\n",
    "print('Load GLOVE embeddings ...')\n",
    "time.sleep(0.5)\n",
    "glove_embeddings = load_embeddings(word_index,\n",
    "                                   \"../input/embeddings/glove.840B.300d/glove.840B.300d.txt\",\n",
    "                                   spell_corrections_glove)\n",
    "\n",
    "print('Load PARAGRAM embeddings ...')\n",
    "time.sleep(0.5)\n",
    "paragram_embeddings = load_embeddings(word_index,\n",
    "                                   \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\",\n",
    "                                   spell_corrections_paragram)\n",
    "\n",
    "embedding_matrix = np.concatenate([glove_embeddings, paragram_embeddings], axis = 1)\n",
    "\n",
    "print('Embedding matrix ready')\n",
    "print('Compute text sequences ...')\n",
    "\n",
    "MAX_WORDS, EMBEDDING_DIM = embedding_matrix.shape\n",
    "MAX_LEN = 55\n",
    "\n",
    "n_train = df[~pd.isnull(df.target)].shape[0]\n",
    "\n",
    "X_train = tokenizer.to_sequences(docs[:n_train], maxlen=MAX_LEN)\n",
    "X_test = tokenizer.to_sequences(docs[n_train:], maxlen=MAX_LEN)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "Y_train = df[~pd.isnull(df.target)].target.values.astype(int)\n",
    "\n",
    "print('Sequences ready')\n",
    "\n",
    "print('Remove garbage ...')\n",
    "del df, docs, glove_embeddings, paragram_embeddings, tokenizer\n",
    "gc.collect()\n",
    "\n",
    "print('Starts training ...\\n')\n",
    "\n",
    "# Grid to inspect for optimal prediction threshold\n",
    "GRID = np.arange(0.9, 0.95, 0.001)\n",
    "\n",
    "N_FOLDS = 8\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True)\n",
    "model_count = 1\n",
    "\n",
    "Y_pred = np.zeros((len(X_test),))\n",
    "\n",
    "# Stores threshold search results\n",
    "df_thresholds = pd.DataFrame(GRID, columns = ['th'])\n",
    "\n",
    "for idx_train, idx_val in skf.split(Y_train, Y_train):\n",
    "    print(\"MODEL:\", model_count)\n",
    "    x_train, y_train = X_train[idx_train], Y_train[idx_train]\n",
    "    x_val, y_val = X_train[idx_val], Y_train[idx_val]\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = get_lstm_model()\n",
    "\n",
    "    hist = model.fit(x_train, y_train, batch_size = 512, epochs=4, verbose=1)\n",
    "                 \n",
    "    preds_val = model.predict(x_val, batch_size=512).squeeze()\n",
    "    preds_val = rankdata(preds_val, method = \"min\")/len(preds_val) # Compute threshold on ranks rather than probs\n",
    "    search_result = threshold_search(y_val, preds_val)\n",
    "    best_th, best_f1 = search_result['threshold'], search_result[\"f1\"]\n",
    "    \n",
    "    print('\\nTh : %.3f  -->  F1 : %.5f \\n' % (best_th, best_f1))\n",
    "    \n",
    "    devs = []\n",
    "    for threshold in GRID:\n",
    "        score = f1_score(y_true=y_val, y_pred=preds_val > threshold)\n",
    "        devs.append(abs(score - best_f1))\n",
    "        \n",
    "    df_thresholds['fold_%d' % model_count] = devs\n",
    "                 \n",
    "    Y_pred += rankdata(model.predict(X_test, batch_size=512).squeeze(), method=\"min\")/len(X_test)\n",
    "        \n",
    "    model_count += 1\n",
    "    \n",
    "print('Finished training!')\n",
    "print('Computes optimal threshold ...')\n",
    "df_thresholds['mean_dev'] = df_thresholds.apply(lambda x : x[1:].mean(), axis = 1, raw=True)\n",
    "opt_threshold = df_thresholds.sort_values('mean_dev').reset_index(drop=True).th[0]\n",
    "print('Found optimal threshold %.3f' % opt_threshold)\n",
    "print('Prepare submission.csv ...')\n",
    "Y_pred_ = Y_pred / N_FOLDS\n",
    "Y_pred_ = (Y_pred_ > opt_threshold).astype(int)\n",
    "\n",
    "sub = pd.read_csv('../input/sample_submission.csv', encoding='utf8')\n",
    "sub['prediction'] = Y_pred_\n",
    "sub.to_csv('submission.csv', encoding='utf8', index=False)\n",
    "\n",
    "S = int(time.time() - t0)\n",
    "M, S = divmod(S, 60)\n",
    "H, M = divmod(M, 60)\n",
    "\n",
    "print('\\n\\n DONE ! Total run-time : {:02d}h{:02d}m{:02d}s'.format(H, M, S))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Now the solution ranks 27th!</h1>\n",
    "<p><img src=\"score_0.70704.png\" width=800></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
